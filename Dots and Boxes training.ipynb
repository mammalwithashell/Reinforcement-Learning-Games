{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib \n",
    "import numpy as np \n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardEnvironment:\n",
    "    \"\"\" this class creates an environment for agents to interact with\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"initialize board\"\n",
    "\n",
    "    def set_players(self, playerA, playerB):\n",
    "        \" connects players with the environment \"\n",
    "        self.playerA = playerA\n",
    "        self.playerB = playerB\n",
    "        self.reset()  # defines current_player\n",
    "        self.score_board = {'X': 0, 'O': 0}\n",
    "\n",
    "    def reset(self):\n",
    "        self.turn = 'X'  # the board always starts with X, regardless of which player\n",
    "\n",
    "        # board states are a 16-character representing the state of the board.\n",
    "#         self.board = list('----------------')\n",
    "        self.board = list('-----------------------')\n",
    "        self.score_board = {'X': 0, 'O': 0}\n",
    "        if (self.playerA and self.playerB):  # if they are set\n",
    "            self.playerA.reset_past()\n",
    "            self.playerB.reset_past()\n",
    "            if (random.random() < 0.5):  # randomly pick the player to start\n",
    "                self.current_player = self.playerA\n",
    "            else:\n",
    "                self.current_player = self.playerB\n",
    "\n",
    "    def print_board(self, board_string=None):\n",
    "        \"print more readable board either from supplied board string or the current board\"\n",
    "        if not board_string:\n",
    "            B = self.board\n",
    "        else:\n",
    "            B = board_string\n",
    "\n",
    "#         print('*',B[0],'*',B[1],'*')\n",
    "#         print(B[2],B[12],B[3],B[13],B[4])\n",
    "#         print('*',B[5],'*',B[6],'*')\n",
    "#         print(B[7],B[14],B[8],B[15],B[9])\n",
    "#         print('*',B[10],'*',B[11],'*')\n",
    "    \n",
    "\n",
    "    def get_state(self):\n",
    "        return \"\".join(self.board)\n",
    "\n",
    "    def other_player(self):\n",
    "        # note, returns other player even if playerA is playing itself\n",
    "        if (self.current_player == self.playerA):\n",
    "            return self.playerB\n",
    "        else:\n",
    "            return self.playerA\n",
    "\n",
    "    def available_actions(self):\n",
    "#         return [ind for ind, val in enumerate(self.board[:12]) if val == '-']\n",
    "        return [ind for ind, val in enumerate(self.board[:17]) if val == '-']\n",
    "\n",
    "    def other_turn(self):\n",
    "        return 'X' if self.turn == 'O' else 'O'\n",
    "\n",
    "    def play_game(self):\n",
    "        # returns the winning player or None if a tie\n",
    "        self.reset()\n",
    "        while True:\n",
    "            choice = self.current_player.select_action()\n",
    "\n",
    "            self.board[choice] = self.turn  # should check if valid\n",
    "\n",
    "            score = self.winner(choice)\n",
    "            self.score_board[self.turn] += len(score)\n",
    "            if len(score)!=0:\n",
    "                #not a tie\n",
    "#                 self.current_player.reward(100)\n",
    "                for i in score:\n",
    "                    self.board[i]=self.turn\n",
    "            else:\n",
    "            # switch players\n",
    "                self.turn = self.other_turn()\n",
    "                self.current_player = self.other_player()\n",
    "\n",
    "            if self.is_full():\n",
    "                break\n",
    "\n",
    "        if self.score_board[self.turn] > self.score_board[self.other_turn()]:\n",
    "            self.current_player.reward(100)\n",
    "            self.other_player().reward(-100)\n",
    "            return self.current_player\n",
    "        elif self.score_board[self.turn] < self.score_board[self.other_turn()]:\n",
    "            self.other_player().reward(100)\n",
    "            self.current_player.reward(-100)\n",
    "            return self.other_player()\n",
    "        else:# it's a tie\n",
    "            self.current_player.reward(0)\n",
    "            self.other_player().reward(0)\n",
    "            return None\n",
    "\n",
    "    def winner(self, choice):\n",
    "        boxes = (\n",
    "#             (0, 2, 3, 5),\n",
    "#             (1, 3, 4, 6),\n",
    "#             (5, 7, 8, 10),\n",
    "#             (6, 8, 9, 11)\n",
    "            (0,3,4,7),\n",
    "            (1,4,5,8),\n",
    "            (2,5,9,6),\n",
    "            (7,10,11,14),\n",
    "            (8,11,12,15),\n",
    "            (9,12,13,16)\n",
    "        )\n",
    "        result=[]\n",
    "        for box in boxes:\n",
    "            if (choice in box) and all(self.board[i] != '-' for i in box):\n",
    "#                 if box == (0,2,3,5):\n",
    "#                     result.append(12)\n",
    "#                 elif box == (1,3,4,6):\n",
    "#                     result.append(13)\n",
    "#                 elif box == (5,7,8,10):\n",
    "#                     result.append(14)\n",
    "#                 else:\n",
    "#                     result.append(15)\n",
    "                if box == (0,3,4,7):\n",
    "                    result.append(17)\n",
    "                elif box == (1,4,5,8):\n",
    "                    result.append(18)\n",
    "                elif box == (2,5,9,6):\n",
    "                    result.append(19)\n",
    "                elif box == (7,10,11,14):\n",
    "                    result.append(20)\n",
    "                elif box == ((8,11,12,15)):\n",
    "                    result.append(21)\n",
    "                else:\n",
    "                    result.append(22)\n",
    "        return result  # if there is no winner\n",
    "\n",
    "    def is_full(self):\n",
    "#         return ('-' not in self.board[0:12])\n",
    "        return ('-' not in self.board[0:17])\n",
    "    # %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" this class is a generic Q-Learning reinforcement learning agent for discrete states and fixed actions\n",
    "    represented as strings\"\"\"\n",
    "    def __init__(self, environment, policy = 'max', learning_rate = 0.5, discount_factor = 0.7, epsilon = 0.6):\n",
    "        if policy in ['max', 'random', 'epsilon']:\n",
    "          self.policy = policy\n",
    "        else:\n",
    "          raise InputError(policy, ' is not an available policy')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.Q = defaultdict(lambda: 0.0) # stores (state, action) value tuples as keys\n",
    "        self.environment = environment\n",
    "        self.epsilon = epsilon # Fraction of time making a random choice for epsilon policy\n",
    "        self.reset_past()\n",
    "\n",
    "    def reset_past(self):\n",
    "      self.past_action = None\n",
    "      self.past_state = None\n",
    "          \n",
    "    def select_action(self):\n",
    "      available_actions = self.environment.available_actions()\n",
    "      if (self.policy == 'random') or (self.policy == 'epsilon' and random.random() < self.epsilon):\n",
    "        choice = random.choice(available_actions)\n",
    "      else: #self.policy == 'max' or it's an epsilon policy determined to pick the max\n",
    "        Q_vals = [self.Q[(self.environment.get_state(), x)] for x in available_actions]\n",
    "        #randomly pick one of the maximum values\n",
    "        max_val = max(Q_vals) # will often be 0 in the beginning\n",
    "        max_pos = [i for i, j in enumerate(Q_vals) if j == max_val]\n",
    "        max_indices = [available_actions[x] for x in max_pos]\n",
    "        choice = random.choice(max_indices)\n",
    "      self.past_state = self.environment.get_state()\n",
    "      self.past_action = choice\n",
    "      return choice\n",
    "        \n",
    "    def reward(self, reward_value):\n",
    "        # finding the best expected reward\n",
    "        available_actions = self.environment.available_actions()\n",
    "        next_Q_vals = [self.Q[(self.environment.get_state(), x)] for x in available_actions]\n",
    "        max_next_Q = max(next_Q_vals) if next_Q_vals else 0 # will often be 0 in the beginning\n",
    "        td_target = reward_value + self.discount_factor * max_next_Q\n",
    "        reward_pred_error = td_target - self.Q[(self.past_state,self.past_action)]\n",
    "        if (self.past_state or self.past_action):\n",
    "          self.Q[(self.past_state,self.past_action)] += self.learning_rate * reward_pred_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class RepeatedGames:\n",
    "    def __init__(self, environment, playerA, playerB):\n",
    "        self.environment = environment\n",
    "        self.playerA = playerA\n",
    "        self.playerB = playerB\n",
    "        self.reset_history()\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def play_game(self):\n",
    "        winner = self.environment.play_game()\n",
    "        if (winner == self.playerA):\n",
    "          self.history.append('A')\n",
    "        elif (winner == self.playerB):\n",
    "          self.history.append('B')\n",
    "        else:\n",
    "          self.history.append('-')\n",
    "    \n",
    "    def play_games(self, games_to_play):\n",
    "        for i in range(games_to_play):\n",
    "            self.play_game()\n",
    "#             sys.stdout.write(\"\\r\")\n",
    "#             sys.stdout.write(\"{:2d} games played.\".format(i))\n",
    "#             sys.stdout.flush()\n",
    "        print(self.history[-games_to_play:].count('A'),'games won by player A')\n",
    "        print(self.history[-games_to_play:].count('B'),'games won by player B')\n",
    "        print(self.history[-games_to_play:].count('-'),'ties')\n",
    "        win_rate=self.history[-games_to_play:].count('A')/len(self.history[-games_to_play:])*100\n",
    "        print(\"Winning rate: {}\".format(win_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 games won by player A\n",
      "38 games won by player B\n",
      "12 ties\n",
      "Winning rate: 50.0\n",
      "\n",
      "441072 games won by player A\n",
      "439776 games won by player B\n",
      "119152 ties\n",
      "Winning rate: 44.1072\n",
      "\n",
      "441323 games won by player A\n",
      "439010 games won by player B\n",
      "119667 ties\n",
      "Winning rate: 44.1323\n",
      "\n",
      "441882 games won by player A\n",
      "438963 games won by player B\n",
      "119155 ties\n",
      "Winning rate: 44.1882\n",
      "\n",
      "443514 games won by player A\n",
      "436969 games won by player B\n",
      "119517 ties\n",
      "Winning rate: 44.351400000000005\n",
      "\n",
      "44 games won by player A\n",
      "47 games won by player B\n",
      "9 ties\n",
      "Winning rate: 44.0\n"
     ]
    }
   ],
   "source": [
    "board = BoardEnvironment()\n",
    "A = Agent(board, 'epsilon')\n",
    "B = Agent(board, 'random')\n",
    "board.set_players(A,B)\n",
    "\n",
    "tournament = RepeatedGames(board,A,B)\n",
    "tournament.play_games(100)\n",
    "print()\n",
    "tournament.play_games(1000000)\n",
    "print()\n",
    "tournament.play_games(1000000)\n",
    "print()\n",
    "tournament.play_games(1000000)\n",
    "print()\n",
    "tournament.play_games(1000000)\n",
    "print()\n",
    "\n",
    "tournament.play_games(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hard1.txt', 'w') as f:\n",
    "    print(dict(A.Q), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the reward hisotry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the history\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "history = np.array(tournament.history.copy())\n",
    "rewards = np.zeros(len(history))\n",
    "rewards[history == 'A'] = 100\n",
    "rewards[history == 'B'] = -100\n",
    "\n",
    "def running_mean(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N, mode='valid')\n",
    "r_mean = running_mean(rewards, 1000)\n",
    "py.plot(r_mean)\n",
    "py.xlabel('games played')\n",
    "py.ylabel('average reward')\n",
    "py.title('Average rewards over 100 games (win/loss is 100/-100)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0b8eaf6f0522fe9eab7ada8b59035839844d32caef7c93a3704490cd2c757a8fc",
   "display_name": "Python 3.9.4  ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "b8eaf6f0522fe9eab7ada8b59035839844d32caef7c93a3704490cd2c757a8fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}